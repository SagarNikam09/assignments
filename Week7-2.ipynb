{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcaa483e-c293-4ca4-b330-abc676b91de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video URLs: []\n"
     ]
    }
   ],
   "source": [
    "#Q1]\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# URL of the YouTube channel's videos page\n",
    "url = 'https://www.youtube.com/@PW-Foundation/videos'\n",
    "\n",
    "# Send a request to the URL\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the first five video links\n",
    "video_links = []\n",
    "for a in soup.find_all('a', {'id': 'video-title'}, limit=5):\n",
    "    video_links.append('https://www.youtube.com' + a['href'])\n",
    "\n",
    "print(\"Video URLs:\", video_links)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "with open('youtube_videos.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Video URL'])\n",
    "    for link in video_links:\n",
    "        writer.writerow([link])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "817fb957-c7e1-4b5f-a060-f52665faf7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thumbnail URLs: []\n"
     ]
    }
   ],
   "source": [
    "#Q2]\n",
    "# Extract the thumbnails URLs of the first five videos\n",
    "thumbnails = []\n",
    "for img in soup.find_all('img', {'id': 'img'}, limit=5):\n",
    "    thumbnails.append(img['src'])\n",
    "\n",
    "print(\"Thumbnail URLs:\", thumbnails)\n",
    "\n",
    "# Save the data to the CSV file\n",
    "with open('youtube_videos.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Thumbnail URL'])\n",
    "    for thumb in thumbnails:\n",
    "        writer.writerow([thumb])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e72ec7e8-ad90-47ec-a379-40cdb5ea914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Titles: []\n"
     ]
    }
   ],
   "source": [
    "#Q3]\n",
    "# Extract the titles of the first five videos\n",
    "titles = []\n",
    "for a in soup.find_all('a', {'id': 'video-title'}, limit=5):\n",
    "    titles.append(a['title'])\n",
    "\n",
    "print(\"Titles:\", titles)\n",
    "\n",
    "# Save the data to the CSV file\n",
    "with open('youtube_videos.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Title'])\n",
    "    for title in titles:\n",
    "        writer.writerow([title])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73f3fc44-c80a-496b-ace6-00f81bda6ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Views: []\n"
     ]
    }
   ],
   "source": [
    "#4]\n",
    "# Extract the number of views of the first five videos\n",
    "views = []\n",
    "for span in soup.find_all('span', {'class': 'style-scope ytd-grid-video-renderer'}, limit=5):\n",
    "    views.append(span.text)\n",
    "\n",
    "print(\"Views:\", views)\n",
    "\n",
    "# Save the data to the CSV file\n",
    "with open('youtube_videos.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Views'])\n",
    "    for view in views:\n",
    "        writer.writerow([view])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "decb958d-ce8a-4dfe-9729-61e2b828b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Times of Posting: []\n"
     ]
    }
   ],
   "source": [
    "#5]\n",
    "# Extract the time of posting of the first five videos\n",
    "times = []\n",
    "for span in soup.find_all('span', {'class': 'style-scope ytd-grid-video-renderer'}, limit=10):\n",
    "    if 'ago' in span.text:\n",
    "        times.append(span.text)\n",
    "    if len(times) == 5:\n",
    "        break\n",
    "\n",
    "print(\"Times of Posting:\", times)\n",
    "\n",
    "# Save the data to the CSV file\n",
    "with open('youtube_videos.csv', 'a', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Time of Posting'])\n",
    "    for time in times:\n",
    "        writer.writerow([time])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39997b52-b2c5-4bf6-aa2f-a7db87131e95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
