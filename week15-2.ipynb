{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0c18b7f-c6c9-438c-859e-737a2308a276",
   "metadata": {},
   "source": [
    "\n",
    "**Q1. R-squared in Linear Regression**\n",
    "\n",
    "* R-squared is a statistical measure representing the proportion of the variance in the dependent variable explained by the independent variables in a regression model. \n",
    "* It's calculated as the ratio of the explained sum of squares (ESS) to the total sum of squares (TSS).\n",
    "* R-squared values range from 0 to 1, where 0 indicates that the model explains none of the variability, and 1 indicates a perfect fit.\n",
    "\n",
    "**Q2. Adjusted R-squared**\n",
    "\n",
    "* Adjusted R-squared is a modified version of R-squared that considers the number of independent variables in the model.\n",
    "* It penalizes the addition of unnecessary variables, preventing overfitting.\n",
    "* Adjusted R-squared is calculated by adjusting the R-squared value based on the number of independent variables and sample size.\n",
    "\n",
    "**Q3. When to Use Adjusted R-squared**\n",
    "\n",
    "* Adjusted R-squared is more appropriate when comparing models with different numbers of independent variables.\n",
    "* It provides a more reliable estimate of the model's predictive power on unseen data.\n",
    "\n",
    "**Q4. RMSE, MSE, and MAE**\n",
    "\n",
    "* **RMSE (Root Mean Squared Error):**  The square root of the average of squared differences between predicted and actual values. It's sensitive to outliers.\n",
    "* **MSE (Mean Squared Error):** The average of squared differences between predicted and actual values. It's sensitive to outliers and gives more weight to larger errors.\n",
    "* **MAE (Mean Absolute Error):** The average of absolute differences between predicted and actual values. It's less sensitive to outliers.\n",
    "\n",
    "**Q5. Pros and Cons of RMSE, MSE, and MAE**\n",
    "\n",
    "| Metric | Advantages | Disadvantages |\n",
    "|---|---|---|\n",
    "| RMSE | Widely used, differentiable, penalizes larger errors | Sensitive to outliers |\n",
    "| MSE | Widely used, differentiable | Sensitive to outliers, more weight to larger errors |\n",
    "| MAE | Robust to outliers, easier to interpret | Not differentiable, less sensitive to larger errors |\n",
    "\n",
    "**Q6. Lasso Regularization**\n",
    "\n",
    "* Lasso regularization adds a penalty term to the loss function that is proportional to the absolute value of the magnitude of coefficients.\n",
    "* It shrinks coefficients towards zero, and some coefficients can become exactly zero, effectively performing feature selection.\n",
    "* Lasso is more appropriate when you want to select a subset of important features and interpret the model.\n",
    "\n",
    "**Q7. Preventing Overfitting with Regularization**\n",
    "\n",
    "* Regularized linear models add a penalty term to the loss function, discouraging the model from fitting the noise in the training data.\n",
    "* This helps the model generalize better to unseen data, reducing overfitting.\n",
    "* For example, using Lasso or Ridge regularization can prevent a model from assigning excessively large coefficients to features, leading to a more stable and generalizable model.\n",
    "\n",
    "**Q8. Limitations of Regularized Linear Models**\n",
    "\n",
    "* Regularized models might underfit if the regularization parameter is too large.\n",
    "* They can be computationally expensive, especially for large datasets.\n",
    "* Regularization might not be effective if the relationship between features and the target variable is highly non-linear.\n",
    "\n",
    "**Q9. Model Comparison with RMSE and MAE**\n",
    "\n",
    "* In this case, Model B with a lower MAE might be preferred as it's less sensitive to outliers.\n",
    "* However, the choice of metric depends on the specific problem and the cost of errors. If larger errors are significantly more costly, RMSE might be a better choice.\n",
    "\n",
    "**Q10. Model Comparison with Ridge and Lasso**\n",
    "\n",
    "* The choice between Ridge and Lasso depends on the desired outcome.\n",
    "* Ridge regression is generally preferred when all features are expected to be relevant and you want to shrink coefficients without necessarily eliminating them.\n",
    "* Lasso is preferred when you want to select a subset of important features and interpret the model.\n",
    "\n",
    "**Trade-offs and Limitations:**\n",
    "\n",
    "* Ridge might not produce sparse solutions (coefficients becoming exactly zero) as Lasso does.\n",
    "* Lasso can be sensitive to the choice of regularization parameter, potentially leading to instability in feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc9fcf7-5b95-423f-9118-ee9429876f1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
