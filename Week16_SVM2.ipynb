{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b1e553-00aa-4da6-ad17-815bab6a3f3e",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\r\n",
    "\r\n",
    "**Relationship**:\r\n",
    "- Polynomial functions can be used as kernel functions in machine learning algorithms like Support Vector Machines (SVM).\r\n",
    "- A polynomial kernel function transforms the input data into a higher-dimensional space, where a linear decision boundary can be applied to classify the data that is not linearly separable in the original space.\r\n",
    "\r\n",
    "**Polynomial Kernel Function**:\r\n",
    "- The polynomial kernel is defined as: `(K(x, y) = (x Â· y + 1)^d)`\r\n",
    "  - Here, `x` and `y` are input vectors, and `d` is the degree of the polynomial.\r\n",
    "- The polynomial kernel allows the algorithm to fit more complex decision boundaries by using higher-degree polynomials.\r\n",
    "\r\n",
    "**Use Case**:\r\n",
    "- Polynomial kernels are particularly useful when you believe the relationship between features and the target variable is non-linear and can be better captured by polynomial combinations of features.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29cae7fe-7458-4062-803a-3a285352ea13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.88      0.93        16\n",
      "           1       0.88      1.00      0.93        14\n",
      "\n",
      "    accuracy                           0.93        30\n",
      "   macro avg       0.94      0.94      0.93        30\n",
      "weighted avg       0.94      0.93      0.93        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, random_state=42)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM model with a polynomial kernel\n",
    "model = SVC(kernel='poly', degree=3, C=1.0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5e3dd1-08c3-4f2b-89df-56058313906f",
   "metadata": {},
   "source": [
    "down\r\n",
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\r\n",
    "\r\n",
    "**Effect of Increasing Epsilon**:\r\n",
    "- In Support Vector Regression (SVR), the `epsilon` parameter defines a margin of tolerance where no penalty is given to errors within this margin.\r\n",
    "- As `epsilon` increases, the margin of tolerance also increases, meaning that more data points can fall within this margin without contributing to the loss function.\r\n",
    "- **Result**: Increasing `epsilon` typically reduces the number of support vectors because fewer data points lie outside the margin and contribute to the model.\r\n",
    "\r\n",
    "**Example**:\r\n",
    "- If `epsilon` is set to a large value, only the data points that lie far outside the margin will be considered as support vectors, simplifying the model but possibly leading to underfitting.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5945e3-e085-4d2b-9940-02094d0575d7",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "** 1. Kernel Function**:\n",
    "- **Role**: Determines the type of decision boundary. Common kernels include linear, polynomial, and RBF (Radial Basis Function).\n",
    "- **Impact**: The choice of kernel affects how the input data is transformed and influences the flexibility of the model.\n",
    "- **When to Adjust**: Use a linear kernel for simple, linear relationships. Use RBF or polynomial kernels for more complex, non-linear relationships.\n",
    "\n",
    "**2. C Parameter**:\n",
    "- **Role**: Controls the trade-off between achieving a low error on the training data and minimizing the margin. It's a regularization parameter.\n",
    "- **Impact**: A high `C` value tries to fit the training data as well as possible, leading to less regularization and potentially overfitting. A low `C` allows a larger margin, leading to more regularization and potentially underfitting.\n",
    "- **When to Adjust**: Increase `C` if the model is underfitting and decrease it if the model is overfitting.\n",
    "\n",
    "**3. Epsilon Parameter**:\n",
    "- **Role**: Defines a margin of tolerance where no penalty is given to errors within this margin.\n",
    "- **Impact**: Increasing `epsilon` creates a wider margin of tolerance, which can simplify the model but may lead to underfitting. Decreasing `epsilon` creates a tighter margin, potentially leading to more support vectors and a more complex model.\n",
    "- **When to Adjust**: Increase `epsilon` when the model is too sensitive to noise and decrease it when the model is underfitting.\n",
    "\n",
    "**4. Gamma Parameter** (specific to RBF kernel):\n",
    "- **Role**: Defines how far the influence of a single training example reaches. It controls the curvature of the decision boundary.\n",
    "- **Impact**: A low `gamma` value implies a more generalized model with smoother boundaries, while a high `gamma` leads to a more complex model with tighter, more localized boundaries.\n",
    "- **When to Adjust**: Increase `gamma` if the model is underfitting and decrease it if the model is overfitting.\n",
    "\n",
    "**Summary**:\n",
    "- The performance of SVR is highly dependent on the choice and tuning of these parameters. It's often necessary to use techniques like cross-validation to find the best combination of parameters for a given dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e4cce2be-a20b-4b4b-aa08-c25f23e63095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n",
      "Precision: 1.00\n",
      "Recall: 1.00\n",
      "F1-Score: 1.00\n",
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=0.1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=0.1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=0.1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=0.1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ...........................C=1, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=1, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=1, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=1, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ..........................C=10, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ........................C=10, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=10, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=10, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .........................C=100, gamma=1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END .......................C=100, gamma=0.1, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END ......................C=100, gamma=0.01, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "[CV] END .....................C=100, gamma=0.001, kernel=rbf; total time=   0.0s\n",
      "Model training complete and saved to 'trained_svc_wine_model.pkl'\n"
     ]
    }
   ],
   "source": [
    "# Q5.\n",
    "# 1. Import the necessary libraries\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.datasets import load_wine\r\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from sklearn.svm import SVC\r\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\r\n",
    "import joblib\r\n",
    "\r\n",
    "# 2. Load the dataset\r\n",
    "wine = load_wine()\r\n",
    "X = wine.data\r\n",
    "y = wine.target\r\n",
    "\r\n",
    "# 3. Split the dataset into training and testing sets\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\r\n",
    "\r\n",
    "# 4. Preprocess the data (e.g., scaling)\r\n",
    "scaler = StandardScaler()\r\n",
    "X_train = scaler.fit_transform(X_train)\r\n",
    "X_test = scaler.transform(X_test)\r\n",
    "\r\n",
    "# 5. Create an instance of the SVC classifier and train it on the training data\r\n",
    "svc = SVC()\r\n",
    "svc.fit(X_train, y_train)\r\n",
    "\r\n",
    "# 6. Use the trained classifier to predict the labels of the testing data\r\n",
    "y_pred = svc.predict(X_test)\r\n",
    "\r\n",
    "# 7. Evaluate the performance of the classifier\r\n",
    "accuracy = accuracy_score(y_test, y_pred)\r\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\r\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\r\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\r\n",
    "\r\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\r\n",
    "print(f\"Precision: {precision:.2f}\")\r\n",
    "print(f\"Recall: {recall:.2f}\")\r\n",
    "print(f\"F1-Score: {f1:.2f}\")\r\n",
    "\r\n",
    "# 8. Tune the hyperparameters of the SVC classifier using GridSearchCV\r\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001], 'kernel': ['rbf']}\r\n",
    "grid_search = GridSearchCV(SVC(), param_grid, refit=True, verbose=2)\r\n",
    "grid_search.fit(X_train, y_train)\r\n",
    "\r\n",
    "# 9. Train the tuned classifier on the entire dataset\r\n",
    "best_svc = grid_search.best_estimator_\r\n",
    "best_svc.fit(X_train, y_train)\r\n",
    "\r\n",
    "# 10. Save the trained classifier to a file for future use\r\n",
    "joblib.dump(best_svc, 'trained_svc_wine_model.pkl')\r\n",
    "\r\n",
    "print(\"Model training complete and saved to 'trained_svc_wine_model.pkl'\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2890be-28ca-4418-bc05-079bacf29621",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
