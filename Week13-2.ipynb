{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0302a407-82d5-4b39-b718-1fec6fc65079",
   "metadata": {},
   "source": [
    "### Q1]\r\n",
    "\r\n",
    "Min-Max scaling is a data preprocessing technique used to rescale features to a fixed range, typically between 0 and 1. This normalization is achieved by applying the following formula to each feature X:\r\n",
    "\r\n",
    "\\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\r\n",
    "\r\n",
    "where:\r\n",
    "- \\( X \\) is an individual data point in the feature.\r\n",
    "- \\( X_{\\text{min}} \\) is the minimum value of the feature.\r\n",
    "- \\( X_{\\text{max}} \\) is the maximum value of the feature.\r\n",
    "\r\n",
    "**Example:**\r\n",
    "\r\n",
    "Consider a dataset with feature values ranging from 50 to 200. Min-Max scaling would transform these values to a range between 0 and 1, preserving the relationships between the original data points.\r\n",
    "oints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaad054-9a44-43a3-9c3c-9c99a6fa8848",
   "metadata": {},
   "source": [
    "### Q2]\n",
    "\n",
    "The Unit Vector technique, also known as normalization, is a feature scaling method that transforms the values of a feature vector to have a length of 1. This is achieved by dividing each component of the vector by its magnitude (or length).\n",
    "\n",
    "**Mathematical Representation:**\n",
    "\n",
    "\\[ X_{\\text{normalized}} = \\frac{X}{||X||} \\]\n",
    "\n",
    "Where:\n",
    "- \\( X \\) is the original feature vector.\n",
    "- \\( ||X|| \\) is the Euclidean norm (magnitude) of \\( X \\).\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a feature vector \\( X = [3, 4] \\). The Euclidean norm of \\( X \\) is \\( ||X|| = \\sqrt{3^2 + 4^2} = 5 \\). Therefore, the normalized vector would be \\( X_{\\text{normalized}} = [\\frac{3}{5}, \\frac{4}{5}] = [0.6, 0.8] \\).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4390623d-1aa0-4a01-b78b-0d97b182d3f7",
   "metadata": {},
   "source": [
    "### Q3]\n",
    "\n",
    "PCA (Principal Component Analysis) is a statistical technique used for dimensionality reduction in machine learning and data analysis. It transforms a large set of potentially correlated variables into a smaller set of uncorrelated variables called principal components. These principal components capture the maximum variance in the original data while being orthogonal to each other.\n",
    "\n",
    "**How PCA works:**\n",
    "\n",
    "1. **Standardization:** The original data is standardized to have zero mean and unit variance. This ensures that all features contribute equally to the analysis, regardless of their original scales.\n",
    "\n",
    "2. **Covariance Matrix Calculation:** The covariance matrix of the standardized data is computed. This matrix captures the relationships between all pairs of features.\n",
    "\n",
    "3. **Eigen Decomposition:** The eigenvectors and eigenvalues of the covariance matrix are calculated. Eigenvectors represent the directions of the principal components, and eigenvalues represent the amount of variance explained by each component.\n",
    "\n",
    "4. **Component Selection:** The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The top \\( k \\) eigenvectors, corresponding to the \\( k \\) largest eigenvalues, are selected as the principal components.\n",
    "\n",
    "5. **Projection:** The original data is projected onto the selected principal components, resulting in a lower-dimensional representation of the data.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset with two features, height and weight. These features might be correlated, meaning that taller people tend to weigh more. PCA can be used to reduce these two features into a single principal component that captures most of the variance in the data. This principal component would represent a combination of height and weight that best explains the overall variability in the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2c2ba-7bce-41eb-ad4c-140f56aea405",
   "metadata": {},
   "source": [
    "###Q4]\n",
    "\n",
    "Feature extraction involves reducing the number of variables in a dataset while retaining the most important information. PCA (Principal Component Analysis) is a popular technique for feature extraction due to its ability to identify significant patterns in data efficiently.\n",
    "\n",
    "**How PCA is used for Feature Extraction:**\n",
    "\n",
    "1. **Dimensionality Reduction:** PCA creates new features (principal components) that are linear combinations of the original features. These principal components are ordered by the amount of variance they explain in the data. By selecting the top few principal components that capture most of the variance, PCA effectively reduces the dimensionality of the data.\n",
    "\n",
    "2. **Decorrelation:** The principal components generated by PCA are uncorrelated, meaning they represent independent sources of information. This property is useful for removing redundancy and multicollinearity in the data.\n",
    "\n",
    "3. **Feature Selection:** PCA can be used as a feature selection technique by choosing the principal components with the highest eigenvalues (variance explained). These components represent the most important features in the dataset.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a dataset of customer purchase history with hundreds of features, such as items purchased, purchase amount, time of purchase, and customer demographics. PCA can be applied to extract the most relevant features that capture key patterns in customer buying behavior. The top few principal components might represent underlying factors like customer preferences for certain product categories, price sensitivity, or purchase frequency. These extracted features can then be used to build more efficient and interpretable predictive models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ce3c84-2a64-4d06-8b2e-54b695761b2c",
   "metadata": {},
   "source": [
    "### Q5]\r\n",
    "\r\n",
    "In a food delivery recommendation system, features like price, rating, and delivery time often have different ranges and units. To ensure these features contribute equally to the recommendation algorithm, Min-Max scaling can be applied to normalize them to a common scale, typically between 0 and 1.\r\n",
    "\r\n",
    "**Steps to Apply Min-Max Scaling:**\r\n",
    "\r\n",
    "1. **Identify the features to be scaled:** In this case, price, rating, and delivery time are the features that need scaling.\r\n",
    "\r\n",
    "2. **Calculate the minimum and maximum values:** For each feature, determine the minimum and maximum values present in the dataset.\r\n",
    "\r\n",
    "3. **Apply the Min-Max scaling formula:** Transform each feature value using the following formula:\r\n",
    "   \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\r\n",
    "   Where:\r\n",
    "   - \\( X_{\\text{scaled}} \\) is the scaled value.\r\n",
    "   - \\( X \\) is the original value.\r\n",
    "   - \\( X_{\\text{min}} \\) is the minimum value of the feature.\r\n",
    "   - \\( X_{\\text{max}} \\) is the maximum value of the feature.\r\n",
    "\r\n",
    "4. **Replace the original features with scaled features:** Use the scaled features in the recommendation algorithm instead of the original features.\r\n",
    "\r\n",
    "**Example:**\r\n",
    "\r\n",
    "Suppose the price of a dish is $25, the minimum price in the dataset is $10, and the maximum price is $50. The scaled price would be calculated as:\r\n",
    "\\[ X_{\\text{scaled}} = \\frac{25 - 10}{50 - 10} = \\frac{15}{40} = 0.375 \\]\r\n",
    "\r\n",
    "**Benefits of using Min-Max scaling in this scenario:**\r\n",
    "\r\n",
    "- **Prevents feature dominance:** Ensures that no single feature dominates the recommendation algorithm due to its scale.\r\n",
    "- **Improves algorithm convergence:** Can help gradient-based optimization algorithms converge faster.\r\n",
    "- **Enhances model performance:** May lead to better recommendation accuracy by providing normalized input to the algorithm.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d97d6cf7-9ab8-4e1f-92e3-2a3d367d9671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Scaled data (-1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Original dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Define the desired range\n",
    "min_range = -1\n",
    "max_range = 1\n",
    "\n",
    "# Calculate Min-Max scaling\n",
    "X_min = np.min(data)\n",
    "X_max = np.max(data)\n",
    "scaled_data = (data - X_min) / (X_max - X_min) * (max_range - min_range) + min_range\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data (-1 to 1):\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee86c71c-2c1c-4ad5-926c-8f1d08b659e2",
   "metadata": {},
   "source": [
    "### Q8]\n",
    "\n",
    "In a stock price prediction project, dealing with a high-dimensional dataset containing various financial indicators and market trends can pose challenges like overfitting, increased computational complexity, and difficulty in interpreting the model. PCA (Principal Component Analysis) can effectively address these issues by reducing the dimensionality of the dataset while preserving the most important information.\n",
    "\n",
    "**Steps to Apply PCA for Dimensionality Reduction:**\n",
    "\n",
    "1. **Data Preparation:**\n",
    "   - Collect and clean the dataset, handling missing values and outliers.\n",
    "   - Normalize the data to ensure that features with different scales contribute equally.\n",
    "\n",
    "2. **PCA Implementation:**\n",
    "   - Apply PCA to the normalized dataset.\n",
    "   - Calculate the covariance matrix to understand the relationships between features.\n",
    "   - Perform eigen decomposition to obtain the eigenvectors (principal components) and their corresponding eigenvalues (explained variance).\n",
    "\n",
    "3. **Component Selection:**\n",
    "   - Sort the principal components based on their eigenvalues in descending order.\n",
    "   - Select the top 'k' principal components that capture a significant portion of the total variance (e.g., 95%). This reduces the dimensionality of the data from potentially hundreds of features to a smaller set of 'k' components.\n",
    "\n",
    "4. **Feature Transformation:**\n",
    "   - Project the original data onto the selected principal components. This transforms the data into a new feature space with reduced dimensions.\n",
    "\n",
    "5. **Model Building:**\n",
    "   - Use the transformed dataset with reduced features to train your stock price prediction model. This can be any suitable machine learning algorithm, such as linear regression, decision trees, or neural networks.\n",
    "\n",
    "**Benefits of using PCA in this context:**\n",
    "- **Reduced Overfitting:** Lowering the number of features helps prevent the model from fitting noise in the original data.\n",
    "- **Improved Computational Efficiency:** Training and testing the model becomes faster with fewer features.\n",
    "- **Enhanced Model Interpretability:** Focusing on the most important principal components can provide insights into the key drivers of stock prices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30d397-186f-4ce4-a9b2-f924761fd5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
