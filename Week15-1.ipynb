{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b46f7c-a763-4fa8-8091-4ffb22975300",
   "metadata": {},
   "source": [
    "Q1]*\r\n",
    "\r\n",
    "**Simple Linear Regression**\r\n",
    "\r\n",
    "* Involves **one independent variable** that predicts the outcome of a **dependent variable**.\r\n",
    "* The relationship is represented by a straight line.\r\n",
    "* The equation is:  y = β0 + β1x + ε \r\n",
    "    * y = Dependent variable\r\n",
    "    * x = Independent variable\r\n",
    "    * β0 = Intercept (value of y when x=0)\r\n",
    "    * β1 = Slope (change in y for a unit change in x)\r\n",
    "    * ε = Error term\r\n",
    "\r\n",
    "**Example:** Predicting house prices based on square footage.\r\n",
    "\r\n",
    "**Multiple Linear Regression**\r\n",
    "\r\n",
    "* Involves **two or more independent variables** to predict the outcome of a **dependent variable**.\r\n",
    "* The relationship is represented by a plane or hyperplane.\r\n",
    "* The equation is: y = β0 + β1x1 + β2x2 + ... + βnxn + ε\r\n",
    "    * y = Dependent variable\r\n",
    "    * x1, x2, ... xn = Independent variables\r\n",
    "    * β0 = Intercept\r\n",
    "    * β1, β2, ... βn = Slopes for each independent variabre details on any of these concepts!rooms, and location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed405848-1988-4b0d-904f-467f47e1e134",
   "metadata": {},
   "source": [
    "Q2]\n",
    "Assumptions of Linear Regression:<br>\n",
    "Linearity: The relationship between the independent and dependent variables is linear.\n",
    "Independence: Observations are independent of each other.<br>\n",
    "Homoscedasticity: The variance of errors is constant across all levels of independent variables.\n",
    "Normality: Errors are normally distributed with a mean of zero.<br>\n",
    "No Multicollinearity: Independent variables are not highly correlated with each other.<br>\n",
    "How to Check the Assumptions:<br>\n",
    "1. Linearity:\n",
    "Scatter Plots: Plot the dependent variable against each independent variable to visually inspect for a linear relationship.\n",
    "Residual Plots: Plot residuals (difference between actual and predicted values) against predicted values. A random pattern suggests linearity, while a pattern indicates non-linearity.\n",
    "2. Independence:\n",
    "Durbin-Watson Test: This test checks for autocorrelation in residuals. A value around 2 indicates no autocorrelation.\n",
    "3. Homoscedasticity:\n",
    "Residual Plots: Check for a constant band of residuals across predicted values. A funnel shape suggests heteroscedasticity.\n",
    "Statistical Tests: Breusch-Pagan or White tests can formally test for heteroscedasticity.\n",
    "4. Normality:\n",
    "Histogram and Q-Q Plot of Residuals: Visually check if residuals follow a normal distribution.\n",
    "Statistical Tests: Shapiro-Wilk or Kolmogorov-Smirnov tests can formally test for normality.\n",
    "5. No Multicollinearity:\n",
    "Correlation Matrix: Calculate the correlation between independent variables. High correlation indicates multicollinearity.\n",
    "Variance Inflation Factor (VIF): VIF quantifies the severity of multicollinearity. A VIF above 5 suggests a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cff81c-d604-4c9b-af94-b7ec8f20ff3b",
   "metadata": {},
   "source": [
    "Q3]\n",
    "*\r\n",
    "\r\n",
    "**Interpretation of Slope and Intercept:**\r\n",
    "\r\n",
    "*   **Slope (β1):** Represents the change in the dependent variable (y) for a one-unit change in the independent variable (x). It shows the direction (positive or negative) and magnitude of the relationship.\r\n",
    "*   **Intercept (β0):** Represents the value of the dependent variable (y) when the independent variable (x) is zero. It's the point where the regression line crosses the y-axis.\r\n",
    "\r\n",
    "**Real-World Example: Sales vs. Advertising Expenditure**\r\n",
    "\r\n",
    "Let's say you run a business and want to model the relationship between advertising expenditure (x) and sales revenue (y). You collect data and fit a linear regression model, obtaining the following equation:\r\n",
    "\r\n",
    "```\r\n",
    "y = 1000 + 5x\r\n",
    "```\r\n",
    "\r\n",
    "**Interpretation:**\r\n",
    "\r\n",
    "*   **Intercept (1000):** This means that even with zero advertising expenditure, you can expect a baseline sales revenue of $1000. This could be due to factors like brand recognition or existing customer base.\r\n",
    "*   **Slope (5):** This means that for every additional dollar spent on advertising, you can expect sales revenue to increase by $5. The positive slope indicates a direct relationship: more advertising leads to higher sales.\r\n",
    "\r\n",
    "**Important Note:** The interpretation of intercept becomes less meaningful if the independent variable (x) doesn't realistically take a value of zero. In our example, it's unlikely to have zero advplanation of these concepts!her sales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea06b0a4-3a5e-47a3-99e9-c2082e3fabbd",
   "metadata": {},
   "source": [
    "Q4]\n",
    "*\r\n",
    "\r\n",
    "**Gradient Descent:**\r\n",
    "\r\n",
    "* Gradient descent is an iterative optimization algorithm used to find the minimum of a function. \r\n",
    "* In machine learning, the function is typically a loss function that measures the error between predicted and actual values.\r\n",
    "* The goal is to find the model parameters (weights and biases) that minimize this error.\r\n",
    "\r\n",
    "**How Gradient Descent Works:**\r\n",
    "\r\n",
    "1. **Initialization:** Start with random values for the model parameters.\r\n",
    "2. **Gradient Calculation:** Calculate the gradient of the loss function with respect to the parameters. The gradient points in the direction of the steepest increase in the loss function.\r\n",
    "3. **Update Parameters:** Update the parameters in the opposite direction of the gradient. This moves the parameters towards the minimum of the loss function.\r\n",
    "4. **Iterate:** Repeat steps 2 and 3 until the parameters converge to a minimum or a maximum number of iterations is reached.\r\n",
    "\r\n",
    "**Gradient Descent in Machine Learning:**\r\n",
    "\r\n",
    "* Gradient descent is widely used in machine learning for training various models, including linear regression, logistic regression, neural networks, and more.\r\n",
    "* It helps these models learn from data by iteratively adjusting their parameters to make more acic machine learning models!\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c099115-31f2-44f9-9894-f9e10ab4090a",
   "metadata": {},
   "source": [
    "Q5]\n",
    "\n",
    "**Multiple Linear Regression Model:**\n",
    "\n",
    "*   Predicts a dependent variable (y) based on two or more independent variables (x1, x2, ... xn).\n",
    "*   Represents the relationship using a plane or hyperplane.\n",
    "*   Equation: y = β0 + β1x1 + β2x2 + ... + βnxn + ε\n",
    "    *   y: Dependent variable\n",
    "    *   x1, x2, ... xn: Independent variables\n",
    "    *   β0: Intercept\n",
    "    *   β1, β2, ... βn: Slopes for each independent variable\n",
    "    *   ε: Error term\n",
    "\n",
    "**Differences from Simple Linear Regression:**\n",
    "\n",
    "| Feature | Simple Linear Regression | Multiple Linear Regression |\n",
    "|---|---|---|\n",
    "| Independent Variables | One (x) | Two or more (x1, x2, ... xn) |\n",
    "| Relationship | Straight line | Plane or hyperplane |\n",
    "| Equation | y = β0 + β1x + ε | y = β0 + β1x1 + β2x2 + ... + βnxn + ε |\n",
    "| Complexity | Simpler | More complex |\n",
    "| Accuracy | Generally less accurate | Generally more accurate |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89918ee3-4ec0-4d03-89a1-b3ad922c8dbc",
   "metadata": {},
   "source": [
    "Q6]\n",
    "\n",
    "**Multicollinearity:**\n",
    "\n",
    "* Occurs when two or more independent variables in a multiple linear regression model are highly correlated.\n",
    "* This correlation makes it difficult to isolate the individual impact of each variable on the dependent variable.\n",
    "* It can lead to unstable and unreliable estimates of regression coefficients.\n",
    "\n",
    "**Detection Methods:**\n",
    "\n",
    "1. **Correlation Matrix:**\n",
    "   * Examine the correlation coefficients between independent variables.\n",
    "   * High correlation (absolute values close to 1) suggests multicollinearity.\n",
    "2. **Variance Inflation Factor (VIF):**\n",
    "   * VIF quantifies how much the variance of a coefficient estimate is inflated due to multicollinearity.\n",
    "   * VIF > 5 generally indicates a problem.\n",
    "3. **Tolerance:**\n",
    "   * Tolerance is the reciprocal of VIF (1 / VIF).\n",
    "   * Tolerance < 0.2 suggests multicollinearity.\n",
    "4. **Eigenvalues:**\n",
    "   * Small eigenvalues of the correlation matrix indicate multicollinearity.\n",
    "\n",
    "**Mitigation Methods:**\n",
    "\n",
    "1. **Remove Correlated Variables:**\n",
    "   * Drop one of the highly correlated variables from the model.\n",
    "   * Choose the variable that is less theoretically important or has a weaker relationship with the dependent variable.\n",
    "2. **Combine Correlated Variables:**\n",
    "   * Create a new variable that represents the combined effect of the correlated variables (e.g., using principal component analysis).\n",
    "3. **Centering or Standardizing:**\n",
    "   * Center the independent variables around their means or standardize them to have zero mean and unit variance.\n",
    "   * This can sometimes reduce the impact of multicollinearity.\n",
    "4. **Ridge Regression or Lasso Regression:**\n",
    "   * These techniques introduce a penalty term in the regression equation to constrain the size of coefficients.\n",
    "   * This can help stabilize estimates in the presence of multicollinearity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47877a21-0516-4aee-806b-a509f12a77bc",
   "metadata": {},
   "source": [
    "Q7]\n",
    "\n",
    "**Polynomial Regression Model:**\n",
    "\n",
    "*   Models non-linear relationships between the independent variable (x) and the dependent variable (y).\n",
    "*   Transforms the independent variable into polynomial terms (e.g., x², x³, etc.) and fits a linear model to these transformed variables.\n",
    "*   Equation: y = β0 + β1x + β2x² + β3x³ + ... + βnxⁿ + ε\n",
    "    *   y: Dependent variable\n",
    "    *   x: Independent variable\n",
    "    *   β0, β1, β2 ... βn: Coefficients for each polynomial term\n",
    "    *   ε: Error term\n",
    "\n",
    "**Differences from Linear Regression:**\n",
    "\n",
    "| Feature           | Linear Regression                                 | Polynomial Regression                                      |\n",
    "| :---------------- | :------------------------------------------------ | :---------------------------------------------------------- |\n",
    "| Relationship      | Linear (straight line)                           | Non-linear (curves)                                        |\n",
    "| Independent Var. | x                                                | x, x², x³, ... xⁿ                                           |\n",
    "| Equation          | y = β0 + β1x + ε                                 | y = β0 + β1x + β2x² + β3x³ + ... + βnxⁿ + ε                 |\n",
    "| Flexibility       | Less flexible                                    | More flexible (can capture complex curves)                 |\n",
    "| Overfitting Risk | Lower                                             | Higher (if the polynomial degree is too high)                |\n",
    "| Interpretation    | Coefficients directly interpretable               | Coefficients less directly interpretable                    |\n",
    "| Use Cases         | Simple relationships                             | Complex relationships, where a linear model is inadequate |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762605ec-4314-46e6-a48b-4b70810c5c28",
   "metadata": {},
   "source": [
    "Q8]\n",
    "*\r\n",
    "\r\n",
    "**Advantages of Polynomial Regression:**\r\n",
    "\r\n",
    "*   **Flexibility:** Captures non-linear relationships, providing a better fit for complex data patterns.\r\n",
    "*   **Improved Accuracy:** Can significantly improve accuracy compared to linear regression when the underlying relationship is non-linear.\r\n",
    "*   **Feature Generation:** Automatically generates polynomial features, potentially uncovering hidden patterns.\r\n",
    "\r\n",
    "**Disadvantages of Polynomial Regression:**\r\n",
    "\r\n",
    "*   **Overfitting:** Prone to overfitting, especially with high-degree polynomials, leading to poor generalization on new data.\r\n",
    "*   **Complexity:** More complex model with multiple coefficients, making interpretation less straightforward.\r\n",
    "*   **Computational Cost:** Can be computationally expensive, especially for high-degree polynomials and large datasets.\r\n",
    "\r\n",
    "**Situations Favoring Polynomial Regression:**\r\n",
    "\r\n",
    "*   **Non-Linear Relationships:** When the relationship between variables is clearly non-linear (e.g., exponential, quadratic).\r\n",
    "*   **Domain Knowledge:** When prior knowledge or theoretical understanding suggests a non-linear relationship.\r\n",
    "*   **Model Selection:** When polynomial regression significantly outperforms linear regression  regression proves inadequate.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbd38b3-ff2f-49a4-b6dd-160abd12e18b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
