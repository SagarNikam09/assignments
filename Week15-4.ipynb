{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b983f565-121a-4b54-81e0-1f605095c99a",
   "metadata": {},
   "source": [
    "### Q1\n",
    "\n",
    "Lasso Regression (Least Absolute Shrinkage and Selection Operator) is a type of linear regression that includes a penalty term equal to the absolute value of the coefficients. This penalty encourages the model to shrink some coefficients to zero, effectively performing feature selection. Unlike Ridge Regression, which only shrinks coefficients but keeps all predictors, Lasso can eliminate irrelevant features.\n",
    "\n",
    "### Q2\n",
    "\n",
    "The main advantage of using Lasso Regression in feature selection is its ability to shrink some coefficients to exactly zero, thus effectively reducing the number of predictors in the model and making it more interpretable and efficient.\n",
    "\n",
    "### Q3\n",
    "\n",
    "The coefficients of a Lasso Regression model represent the change in the response variable for a one-unit change in the predictor variable, with some coefficients potentially being exactly zero. This zeroing out indicates that those predictors are not contributing to the model.\n",
    "\n",
    "### Q4\n",
    "\n",
    "The primary tuning parameter in Lasso Regression is the regularization parameter (\\(\\lambda\\)). Increasing \\(\\lambda\\) increases the penalty on the coefficients, leading to more coefficients being shrunk to zero, thereby enhancing feature selection. However, too large a \\(\\lambda\\) can oversimplify the model and reduce its predictive power.\n",
    "\n",
    "### Q5\n",
    "\n",
    "Lasso Regression can be adapted for non-linear regression problems by combining it with basis expansions or kernel methods. For instance, polynomial features or splines can be used to capture non-linear relationships before applying Lasso Regression.\n",
    "\n",
    "### Q6\n",
    "\n",
    "The difference between Ridge Regression and Lasso Regression lies in the penalty applied to the coefficients. Ridge Regression adds a penalty equal to the square of the magnitude of the coefficients, shrinking them but not eliminating any. Lasso Regression adds a penalty equal to the absolute value of the coefficients, which can shrink some to zero, effectively selecting features.\n",
    "\n",
    "### Q7\n",
    "\n",
    "Lasso Regression can handle multicollinearity by selecting one predictor from a group of highly correlated predictors and shrinking the others to zero. This helps in reducing redundancy and improving model interpretability.\n",
    "\n",
    "### Q8\n",
    "\n",
    "The optimal value of the regularization parameter (\\(\\lambda\\)) in Lasso Regression is typically chosen using cross-validation. The data is split into training and validation sets, and the model is fitted with various \\(\\lambda\\) values. The value that minimizes the validation error is selected as the optimal \\(\\lambda\\).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41525d5-aea4-4404-921d-f6c41eff0066",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
